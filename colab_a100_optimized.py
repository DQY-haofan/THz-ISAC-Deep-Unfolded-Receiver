# =============================================================================
# ğŸš€ GA-BV-Net A100 æé™è®­ç»ƒé…ç½®
# =============================================================================
#
# A100 ä¼˜åŠ¿ï¼š
# - 40GB/80GB æ˜¾å­˜ â†’ è¶…å¤§ batch size
# - Tensor Core â†’ æ··åˆç²¾åº¦ 2x åŠ é€Ÿ
# - é«˜å¸¦å®½ â†’ æ›´å¿«æ•°æ®ä¼ è¾“
#
# é¢„è®¡è®­ç»ƒæ—¶é—´ï¼š10000 æ­¥ â‰ˆ 10-15 åˆ†é’Ÿ (A100)
# =============================================================================

# %%
# =============================================================================
# CELL 1: ç¯å¢ƒæ£€æŸ¥
# =============================================================================

import torch

print(f"PyTorch: {torch.__version__}")
print(f"CUDA: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"GPU: {gpu_name}")
    print(f"Memory: {gpu_mem:.1f} GB")

    # æ£€æµ‹ A100 ç±»å‹
    if "A100" in gpu_name:
        if gpu_mem > 45:
            print("âœ… A100-80GB detected!")
            A100_TYPE = "80GB"
        else:
            print("âœ… A100-40GB detected!")
            A100_TYPE = "40GB"
    else:
        print(f"âš ï¸ Not A100, but {gpu_name}")
        A100_TYPE = "OTHER"

# %%
# =============================================================================
# CELL 2: A100 ä¼˜åŒ–é…ç½®
# =============================================================================

# æ ¹æ® A100 ç±»å‹é€‰æ‹©é…ç½®
if A100_TYPE == "80GB":
    A100_CONFIG = {
        # === è¶…å¤§ Batch Size ===
        "batch_size": 512,  # 80GB å¯ä»¥ç”¨ 512
        "grad_accumulation": 1,  # ä¸éœ€è¦ç´¯ç§¯

        # === è®­ç»ƒæ­¥æ•° ===
        "n_steps": 15000,  # æ›´å¤šæ­¥æ•°

        # === å­¦ä¹ ç‡ (å¤§batchéœ€è¦æ›´é«˜lr) ===
        "lr_main": 1e-3,  # 3x æé«˜
        "lr_denoiser": 3e-3,  # Denoiser æ›´æ¿€è¿›
        "warmup_steps": 500,

        # === æ¨¡å‹å®¹é‡ (å¯é€‰æ‰©å¤§) ===
        "vamp_layers": 8,  # æ›´å¤šå±‚
        "denoiser_hidden": 512,  # æ›´å®½
    }
elif A100_TYPE == "40GB":
    A100_CONFIG = {
        "batch_size": 256,  # 40GB ç”¨ 256
        "grad_accumulation": 2,  # ç´¯ç§¯åˆ°æœ‰æ•ˆ 512
        "n_steps": 15000,
        "lr_main": 8e-4,
        "lr_denoiser": 2e-3,
        "warmup_steps": 500,
        "vamp_layers": 8,
        "denoiser_hidden": 384,
    }
else:
    # å…¶ä»– GPU çš„ä¿å®ˆé…ç½®
    A100_CONFIG = {
        "batch_size": 64,
        "grad_accumulation": 4,
        "n_steps": 10000,
        "lr_main": 3e-4,
        "lr_denoiser": 1e-3,
        "warmup_steps": 300,
        "vamp_layers": 6,
        "denoiser_hidden": 256,
    }

# é€šç”¨é…ç½®
A100_CONFIG.update({
    # === æ··åˆç²¾åº¦ (å…³é”®åŠ é€Ÿ!) ===
    "use_amp": True,  # è‡ªåŠ¨æ··åˆç²¾åº¦

    # === æŸå¤±æƒé‡ ===
    "denoiser_loss_weight": 5.0,
    "tau_loss_weight": 0.1,

    # === è¯¾ç¨‹å­¦ä¹  ===
    "snr_start": 28,  # ä»æ›´é«˜ SNR å¼€å§‹
    "snr_end": 8,  # é™åˆ°æ›´ä½
    "curriculum_steps": 5000,

    # === æ­£åˆ™åŒ– ===
    "weight_decay": 1e-4,
    "grad_clip": 5.0,
    "dropout": 0.1,

    # === æ•°æ®åŠ è½½ ===
    "num_workers": 4,
    "pin_memory": True,

    # === ä¿å­˜ ===
    "save_every": 2500,
    "log_every": 50,
})

print("\n" + "=" * 60)
print("ğŸ”§ A100 ä¼˜åŒ–é…ç½®")
print("=" * 60)
for k, v in A100_CONFIG.items():
    print(f"  {k}: {v}")

# %%
# =============================================================================
# CELL 3: å¯¼å…¥å’Œè®¾å¤‡è®¾ç½®
# =============================================================================

import os
import sys
import time
import numpy as np
from pathlib import Path
from tqdm.auto import tqdm
import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler

# è®¾ç½® CUDA ä¼˜åŒ–
torch.backends.cudnn.benchmark = True  # è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜ç®—æ³•
torch.backends.cuda.matmul.allow_tf32 = True  # å¯ç”¨ TF32
torch.backends.cudnn.allow_tf32 = True

device = torch.device("cuda")
print(f"\nâœ… Using device: {device}")
print(f"  cuDNN benchmark: enabled")
print(f"  TF32: enabled")

# %%
# =============================================================================
# CELL 4: å¯¼å…¥é¡¹ç›®æ¨¡å—
# =============================================================================

# ç¡®ä¿é¡¹ç›®åœ¨è·¯å¾„ä¸­
# sys.path.insert(0, '/content/project')

from gabv_net_model import GABVNet, GABVNetConfig
from thz_isac_world import SimConfig, THzISACWorld

print("âœ… é¡¹ç›®æ¨¡å—å¯¼å…¥æˆåŠŸ")

# %%
# =============================================================================
# CELL 5: åˆ›å»ºæ¨¡å‹ (A100 ä¼˜åŒ–ç‰ˆ)
# =============================================================================

cfg = A100_CONFIG

# æ¨¡å‹é…ç½® - å¯ä»¥æ›´å¤§
model_config = GABVNetConfig(
    N=1024,
    pilot_len=128,
    vamp_layers=cfg['vamp_layers'],
    tau_gn_iters=3,
    denoiser_hidden=cfg['denoiser_hidden'],
)

model = GABVNet(model_config).to(device)

# ç¼–è¯‘æ¨¡å‹ (PyTorch 2.0+, é¢å¤– 10-30% åŠ é€Ÿ)
if hasattr(torch, 'compile'):
    print("ğŸ”¥ ä½¿ç”¨ torch.compile() åŠ é€Ÿ...")
    model = torch.compile(model, mode="reduce-overhead")

# ç»Ÿè®¡å‚æ•°
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"\nğŸ“Š æ¨¡å‹å‚æ•°:")
print(f"  Total: {total_params:,}")
print(f"  Trainable: {trainable_params:,}")

# åŠ è½½ç°æœ‰ checkpoint (å¯é€‰)
existing_ckpt = "results/checkpoints/Stage3_FullTrak_1766361144/final.pth"
if os.path.exists(existing_ckpt):
    print(f"\nğŸ“¦ åŠ è½½ç°æœ‰ checkpoint: {existing_ckpt}")
    ckpt = torch.load(existing_ckpt, map_location=device)
    # å¤„ç†å¯èƒ½çš„ compile åŒ…è£…
    state_dict = ckpt.get('model_state_dict', ckpt)
    model.load_state_dict(state_dict, strict=False)
    print("âœ… æƒé‡åŠ è½½å®Œæˆ")

# %%
# =============================================================================
# CELL 6: ä¼˜åŒ–å™¨è®¾ç½® (A100 ä¼˜åŒ–)
# =============================================================================

# åˆ†ç»„å‚æ•°
denoiser_params = []
other_params = []

for name, param in model.named_parameters():
    if 'denoiser' in name.lower() or 'vamp' in name.lower():
        denoiser_params.append(param)
    else:
        other_params.append(param)

print(f"\nâš™ï¸ å‚æ•°åˆ†ç»„:")
print(f"  Denoiser: {sum(p.numel() for p in denoiser_params):,} params @ lr={cfg['lr_denoiser']}")
print(f"  Other: {sum(p.numel() for p in other_params):,} params @ lr={cfg['lr_main']}")

# ä½¿ç”¨ AdamW with fused=True (A100 ä¼˜åŒ–)
optimizer = optim.AdamW([
    {'params': other_params, 'lr': cfg['lr_main']},
    {'params': denoiser_params, 'lr': cfg['lr_denoiser']},
], weight_decay=cfg['weight_decay'], fused=True)  # fused=True æ›´å¿«!


# å­¦ä¹ ç‡è°ƒåº¦
def lr_lambda(step):
    if step < cfg['warmup_steps']:
        return step / cfg['warmup_steps']
    progress = (step - cfg['warmup_steps']) / (cfg['n_steps'] - cfg['warmup_steps'])
    return 0.01 + 0.99 * 0.5 * (1 + np.cos(np.pi * progress))


scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

# æ··åˆç²¾åº¦ Scaler
scaler = GradScaler() if cfg['use_amp'] else None

print(f"\nâœ… ä¼˜åŒ–å™¨è®¾ç½®å®Œæˆ")
print(f"  Mixed Precision (AMP): {'Enabled' if cfg['use_amp'] else 'Disabled'}")

# %%
# =============================================================================
# CELL 7: æ•°æ®ç”Ÿæˆå™¨ (ä¼˜åŒ–ç‰ˆ)
# =============================================================================

sim_config = SimConfig(
    fc=300e9,
    fs=10e9,
    N=1024,
    enable_pa=True,
    enable_pn=True,
    enable_quantization=True,
    snr_db=20.0,
)

world = THzISACWorld(sim_config)


def generate_batch_cuda(batch_size, snr_db):
    """ç›´æ¥ç”Ÿæˆ CUDA tensor çš„æ‰¹æ¬¡"""
    sim_config.snr_db = snr_db
    batch = world.generate_batch(batch_size)

    return {
        'y': torch.from_numpy(batch['y']).to(device, non_blocking=True),
        'x': torch.from_numpy(batch['x']).to(device, non_blocking=True),
        'tau': torch.from_numpy(batch['tau']).to(device, non_blocking=True),
        'pilots': torch.from_numpy(batch['pilots']).to(device, non_blocking=True),
        'pilot_idx': torch.from_numpy(batch['pilot_idx']).to(device, non_blocking=True),
    }


print("âœ… æ•°æ®ç”Ÿæˆå™¨å°±ç»ª")

# %%
# =============================================================================
# CELL 8: è®­ç»ƒå¾ªç¯ (A100 æé™ç‰ˆ)
# =============================================================================

print("\n" + "=" * 60)
print("ğŸš€ A100 æé™è®­ç»ƒå¼€å§‹")
print("=" * 60)
print(f"  Batch size: {cfg['batch_size']}")
print(f"  Grad accumulation: {cfg['grad_accumulation']}")
print(f"  Effective batch: {cfg['batch_size'] * cfg['grad_accumulation']}")
print(f"  Total steps: {cfg['n_steps']}")
print(f"  Mixed Precision: {cfg['use_amp']}")
print("=" * 60)

# è¾“å‡ºç›®å½•
output_dir = Path("results/a100_intensive")
output_dir.mkdir(parents=True, exist_ok=True)

# è®­ç»ƒè®°å½•
history = {
    'step': [], 'loss': [], 'det_loss': [], 'denoiser_loss': [],
    'lr': [], 'snr': [], 'gpu_mem': [], 'throughput': [],
}

best_loss = float('inf')
start_time = time.time()
tokens_processed = 0

model.train()

# é¢„çƒ­ GPU
print("\nğŸ”¥ é¢„çƒ­ GPU...")
for _ in range(3):
    dummy_batch = generate_batch_cuda(cfg['batch_size'], 20.0)
    with autocast(enabled=cfg['use_amp']):
        _ = model(dummy_batch['y'], dummy_batch['pilots'], dummy_batch['pilot_idx'])
torch.cuda.synchronize()
print("âœ… GPU é¢„çƒ­å®Œæˆ")

# ä¸»è®­ç»ƒå¾ªç¯
pbar = tqdm(range(1, cfg['n_steps'] + 1), desc="Training", ncols=120)

for step in pbar:
    step_start = time.time()

    # === è¯¾ç¨‹å­¦ä¹  SNR ===
    if step < cfg['curriculum_steps']:
        progress = step / cfg['curriculum_steps']
        current_snr = cfg['snr_start'] - progress * (cfg['snr_start'] - cfg['snr_end'])
    else:
        current_snr = np.random.uniform(cfg['snr_end'], cfg['snr_start'])

    # === æ¢¯åº¦ç´¯ç§¯å¾ªç¯ ===
    optimizer.zero_grad(set_to_none=True)  # æ›´é«˜æ•ˆçš„æ¸…é›¶

    accumulated_loss = 0.0
    accumulated_det = 0.0
    accumulated_den = 0.0

    for micro_step in range(cfg['grad_accumulation']):
        # ç”Ÿæˆæ•°æ®
        batch = generate_batch_cuda(cfg['batch_size'], current_snr)

        # æ··åˆç²¾åº¦å‰å‘
        with autocast(enabled=cfg['use_amp']):
            outputs = model(batch['y'], batch['pilots'], batch['pilot_idx'])

            # æ£€æµ‹æŸå¤±
            det_loss = nn.functional.mse_loss(outputs['x_hat'], batch['x'])

            # Ï„ æŸå¤±
            tau_loss = nn.functional.mse_loss(outputs['tau_hat'], batch['tau'])

            # Denoiser ä¸­é—´å±‚æŸå¤±
            denoiser_loss = torch.tensor(0.0, device=device)
            if 'intermediate_x' in outputs and outputs['intermediate_x']:
                for i, x_int in enumerate(outputs['intermediate_x']):
                    weight = 0.5 ** (len(outputs['intermediate_x']) - i - 1)
                    denoiser_loss = denoiser_loss + weight * nn.functional.mse_loss(x_int, batch['x'])

            # æ€»æŸå¤± (é™¤ä»¥ç´¯ç§¯æ­¥æ•°)
            loss = (det_loss + cfg['tau_loss_weight'] * tau_loss +
                    cfg['denoiser_loss_weight'] * denoiser_loss) / cfg['grad_accumulation']

        # åå‘ä¼ æ’­ (æ··åˆç²¾åº¦)
        if scaler:
            scaler.scale(loss).backward()
        else:
            loss.backward()

        accumulated_loss += loss.item() * cfg['grad_accumulation']
        accumulated_det += det_loss.item()
        accumulated_den += denoiser_loss.item() if isinstance(denoiser_loss, torch.Tensor) else denoiser_loss

    # === ä¼˜åŒ–å™¨æ­¥è¿› ===
    if scaler:
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg['grad_clip'])
        scaler.step(optimizer)
        scaler.update()
    else:
        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg['grad_clip'])
        optimizer.step()

    scheduler.step()

    # === ç»Ÿè®¡ ===
    step_time = time.time() - step_start
    tokens_processed += cfg['batch_size'] * cfg['grad_accumulation'] * 1024
    throughput = cfg['batch_size'] * cfg['grad_accumulation'] / step_time
    gpu_mem = torch.cuda.max_memory_allocated() / 1e9

    # è®°å½•
    history['step'].append(step)
    history['loss'].append(accumulated_loss)
    history['det_loss'].append(accumulated_det / cfg['grad_accumulation'])
    history['denoiser_loss'].append(accumulated_den / cfg['grad_accumulation'])
    history['lr'].append(scheduler.get_last_lr()[1])
    history['snr'].append(current_snr)
    history['gpu_mem'].append(gpu_mem)
    history['throughput'].append(throughput)

    # === è¿›åº¦æ¡æ›´æ–° ===
    pbar.set_postfix({
        'loss': f"{accumulated_loss:.4f}",
        'det': f"{accumulated_det / cfg['grad_accumulation']:.4f}",
        'den': f"{accumulated_den / cfg['grad_accumulation']:.4f}",
        'lr': f"{scheduler.get_last_lr()[1]:.1e}",
        'SNR': f"{current_snr:.0f}",
        'mem': f"{gpu_mem:.1f}G",
        'spd': f"{throughput:.0f}/s",
    })

    # === è¯¦ç»†æ—¥å¿— ===
    if step % cfg['log_every'] == 0:
        elapsed = time.time() - start_time
        eta = elapsed / step * (cfg['n_steps'] - step)

        tqdm.write(
            f"\n[{step:5d}/{cfg['n_steps']}] "
            f"Loss: {accumulated_loss:.4f} | "
            f"Det: {accumulated_det / cfg['grad_accumulation']:.4f} | "
            f"Den: {accumulated_den / cfg['grad_accumulation']:.4f} | "
            f"LR: {scheduler.get_last_lr()[1]:.2e} | "
            f"SNR: {current_snr:.1f} | "
            f"GPU: {gpu_mem:.1f}GB | "
            f"Speed: {throughput:.0f} samples/s | "
            f"ETA: {eta / 60:.1f}min"
        )

        # Denoiser æƒé‡ç›‘æ§
        denoiser_norm = sum(p.data.norm().item() for p in denoiser_params)
        tqdm.write(f"  Denoiser weight norm: {denoiser_norm:.4f}")

    # === ä¿å­˜ ===
    if step % cfg['save_every'] == 0 or step == cfg['n_steps']:
        checkpoint = {
            'step': step,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'scaler_state_dict': scaler.state_dict() if scaler else None,
            'config': cfg,
            'history': history,
            'loss': accumulated_loss,
        }

        save_path = output_dir / f"step_{step}.pth"
        torch.save(checkpoint, save_path)

        if accumulated_loss < best_loss:
            best_loss = accumulated_loss
            torch.save(checkpoint, output_dir / "best.pth")
            tqdm.write(f"  ğŸ’¾ New best! Loss: {best_loss:.4f}")

# %%
# =============================================================================
# CELL 9: è®­ç»ƒå®Œæˆç»Ÿè®¡
# =============================================================================

total_time = time.time() - start_time
avg_throughput = np.mean(history['throughput'])

print("\n" + "=" * 60)
print("âœ… è®­ç»ƒå®Œæˆ!")
print("=" * 60)
print(f"  æ€»æ—¶é—´: {total_time / 60:.1f} åˆ†é’Ÿ")
print(f"  å¹³å‡é€Ÿåº¦: {avg_throughput:.0f} samples/s")
print(f"  GPU å³°å€¼å†…å­˜: {max(history['gpu_mem']):.1f} GB")
print(f"  æœ€ä½³ Loss: {best_loss:.4f}")
print(f"  æœ€ç»ˆ Loss: {history['loss'][-1]:.4f}")

# ä¿å­˜æœ€ç»ˆæ¨¡å‹
final_ckpt = {
    'step': cfg['n_steps'],
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'config': cfg,
    'history': history,
    'training_time': total_time,
}
torch.save(final_ckpt, output_dir / "final.pth")
print(f"\nğŸ“¦ æœ€ç»ˆæ¨¡å‹ä¿å­˜åˆ°: {output_dir / 'final.pth'}")

# %%
# =============================================================================
# CELL 10: Denoiser æƒé‡è¯Šæ–­
# =============================================================================

print("\n" + "=" * 60)
print("ğŸ”¬ Denoiser æƒé‡è¯Šæ–­ (è®­ç»ƒå)")
print("=" * 60)

model.eval()

for name, param in model.named_parameters():
    if 'denoiser' in name.lower() or 'vamp' in name.lower():
        norm = param.data.norm().item()
        std = param.data.std().item()
        mean = param.data.mean().item()
        max_val = param.data.abs().max().item()
        print(f"{name}:")
        print(f"  norm={norm:.4f}, std={std:.6f}, mean={mean:.6f}, max={max_val:.4f}")

# %%
# =============================================================================
# CELL 11: å¿«é€ŸéªŒè¯
# =============================================================================

print("\n" + "=" * 60)
print("ğŸ§ª å¿«é€ŸéªŒè¯")
print("=" * 60)

model.eval()
with torch.no_grad():
    for test_snr in [10, 15, 20, 25]:
        test_batch = generate_batch_cuda(128, test_snr)

        with autocast(enabled=cfg['use_amp']):
            outputs = model(test_batch['y'], test_batch['pilots'], test_batch['pilot_idx'])

        # è®¡ç®— MSE
        mse = nn.functional.mse_loss(outputs['x_hat'], test_batch['x']).item()
        tau_rmse = torch.sqrt(nn.functional.mse_loss(outputs['tau_hat'], test_batch['tau'])).item()

        # è®¡ç®— SER (è¿‘ä¼¼)
        x_hat = outputs['x_hat']
        x_true = test_batch['x']
        x_hat_hard = torch.sign(x_hat.real) + 1j * torch.sign(x_hat.imag)
        x_hat_hard = x_hat_hard / np.sqrt(2)
        x_true_hard = torch.sign(x_true.real) + 1j * torch.sign(x_true.imag)
        x_true_hard = x_true_hard / np.sqrt(2)
        ser = (x_hat_hard != x_true_hard).float().mean().item()

        print(f"  SNR={test_snr:2d} dB: MSE={mse:.4f}, Ï„_RMSE={tau_rmse:.4f}, SERâ‰ˆ{ser:.4f}")

# %%
# =============================================================================
# CELL 12: ç»˜åˆ¶è®­ç»ƒæ›²çº¿
# =============================================================================

import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 3, figsize=(15, 8))

# Loss
ax = axes[0, 0]
ax.semilogy(history['step'], history['loss'], 'b-', alpha=0.7)
ax.set_xlabel('Step')
ax.set_ylabel('Total Loss')
ax.set_title('Training Loss')
ax.grid(True, alpha=0.3)

# Detection Loss
ax = axes[0, 1]
ax.semilogy(history['step'], history['det_loss'], 'g-', alpha=0.7)
ax.set_xlabel('Step')
ax.set_ylabel('Detection Loss')
ax.set_title('Detection Loss')
ax.grid(True, alpha=0.3)

# Denoiser Loss
ax = axes[0, 2]
ax.semilogy(history['step'], history['denoiser_loss'], 'r-', alpha=0.7)
ax.set_xlabel('Step')
ax.set_ylabel('Denoiser Loss')
ax.set_title('Denoiser Loss')
ax.grid(True, alpha=0.3)

# Learning Rate
ax = axes[1, 0]
ax.semilogy(history['step'], history['lr'], 'purple', alpha=0.7)
ax.set_xlabel('Step')
ax.set_ylabel('Learning Rate')
ax.set_title('Learning Rate Schedule')
ax.grid(True, alpha=0.3)

# SNR Curriculum
ax = axes[1, 1]
ax.plot(history['step'], history['snr'], 'orange', alpha=0.7)
ax.set_xlabel('Step')
ax.set_ylabel('SNR (dB)')
ax.set_title('SNR Curriculum')
ax.grid(True, alpha=0.3)

# Throughput
ax = axes[1, 2]
ax.plot(history['step'], history['throughput'], 'cyan', alpha=0.7)
ax.set_xlabel('Step')
ax.set_ylabel('Samples/s')
ax.set_title('Training Throughput')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(output_dir / "training_curves.png", dpi=150)
plt.show()

print(f"\nğŸ“Š è®­ç»ƒæ›²çº¿ä¿å­˜åˆ°: {output_dir / 'training_curves.png'}")

# %%
# =============================================================================
# CELL 13: ä¸‹è½½ç»“æœ
# =============================================================================

import shutil
from google.colab import files

# æ‰“åŒ…
archive_name = "a100_training_results"
shutil.make_archive(archive_name, 'zip', str(output_dir))

# ä¸‹è½½
files.download(f"{archive_name}.zip")

print("âœ… ä¸‹è½½å®Œæˆ!")
print(f"\nè§£å‹åï¼Œå°† final.pth å¤åˆ¶åˆ°æœ¬åœ°é¡¹ç›®:")
print(f"  cp final.pth results/checkpoints/Stage3_FullTrak_1766361144/final.pth")